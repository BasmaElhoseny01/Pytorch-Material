{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading Model\n",
    "\n",
    "https://www.youtube.com/watch?v=9L9jEOwRrCg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,n_input_features):\n",
    "        super(Model,self).__init__()\n",
    "        self.linear= nn.Linear(n_input_features,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y_pred=torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Parameters of Model\n",
      "OrderedDict([('linear.weight', tensor([[-0.0284, -0.0576, -0.4028,  0.2033, -0.2339, -0.1442]])), ('linear.bias', tensor([0.1883]))])\n",
      "Parameter containing:\n",
      "tensor([[-0.0284, -0.0576, -0.4028,  0.2033, -0.2339, -0.1442]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1883], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n_input_features=6\n",
    "model=Model(n_input_features=n_input_features)\n",
    "\n",
    "print(\"Old Parameters of Model\")\n",
    "old_state = copy.deepcopy(model.state_dict())\n",
    "print(old_state)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples,n_features 10 6\n",
      "Training ......\n",
      "Epoch:0\n",
      "Epoch:1\n",
      "epoch: 2, loss = 18550.6465\n",
      "Epoch:2\n",
      "Epoch:3\n",
      "epoch: 4, loss = 18506.3066\n",
      "Epoch:4\n",
      "Epoch:5\n",
      "epoch: 6, loss = 18492.7148\n",
      "Epoch:6\n",
      "Epoch:7\n",
      "epoch: 8, loss = 18486.6367\n",
      "Epoch:8\n",
      "Epoch:9\n",
      "epoch: 10, loss = 18483.2656\n",
      "Old Parameters of Model OrderedDict([('linear.weight', tensor([[-0.0284, -0.0576, -0.4028,  0.2033, -0.2339, -0.1442]])), ('linear.bias', tensor([0.1883]))])\n",
      "New Parameters of Model OrderedDict([('linear.weight', tensor([[ 1.0419, -0.2924,  1.2426,  1.2473, -0.0870,  1.6466]])), ('linear.bias', tensor([-0.6256]))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 0)Prepare Data\n",
    "X_numpy,y_numpy=datasets.make_regression(n_samples=10,n_features=n_input_features,noise=20,random_state=1)\n",
    "\n",
    "x=torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0],1) # col vector\n",
    "\n",
    "n_samples,n_features=x.shape\n",
    "print(\"n_samples,n_features\",n_samples,n_features)\n",
    "\n",
    "\n",
    "print(\"Training ......\")\n",
    "learning_rate=0.01\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "# Training \n",
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch:{epoch}')\n",
    "    # Forward Pass and loss\n",
    "    y_predicted=model(x)\n",
    "    loss=criterion(y_predicted,y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%2==0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(\"Old Parameters of Model\",old_state)\n",
    "model_trained_state=copy.deepcopy(model.state_dict())\n",
    "print(\"New Parameters of Model\",model_trained_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method(1) Lazy Method: Save Whole Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Model Saved :D\n",
      "Loading ...\n",
      "Model Loaded :D\n",
      "Old State: OrderedDict([('linear.weight', tensor([[-0.0284, -0.0576, -0.4028,  0.2033, -0.2339, -0.1442]])), ('linear.bias', tensor([0.1883]))])\n",
      "Saved Sate: OrderedDict([('linear.weight', tensor([[ 1.0419, -0.2924,  1.2426,  1.2473, -0.0870,  1.6466]])), ('linear.bias', tensor([-0.6256]))])\n",
      "Loaded State: OrderedDict([('linear.weight', tensor([[ 1.0419, -0.2924,  1.2426,  1.2473, -0.0870,  1.6466]])), ('linear.bias', tensor([-0.6256]))])\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_PATH='./Saved_models/model.pth'\n",
    "\n",
    "# Save\n",
    "print(\"Saving ...\")\n",
    "torch.save(model,MODEL_FILE_PATH)\n",
    "print(\"Model Saved :D\")\n",
    "\n",
    "# Load\n",
    "print(\"Loading ...\")\n",
    "model_loaded_1=torch.load(MODEL_FILE_PATH)\n",
    "print(\"Model Loaded :D\")\n",
    "\n",
    "print(\"Old State:\",old_state)\n",
    "print(\"Saved Sate:\",model_trained_state)\n",
    "print(\"Loaded State:\",model_loaded_1.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method(2): Save State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Model Saved :D\n",
      "Loading ...\n",
      "Model Loaded :D\n",
      "Old State: OrderedDict([('linear.weight', tensor([[-0.0284, -0.0576, -0.4028,  0.2033, -0.2339, -0.1442]])), ('linear.bias', tensor([0.1883]))])\n",
      "Saved Sate: OrderedDict([('linear.weight', tensor([[ 1.0419, -0.2924,  1.2426,  1.2473, -0.0870,  1.6466]])), ('linear.bias', tensor([-0.6256]))])\n",
      "Loaded State: OrderedDict([('linear.weight', tensor([[ 1.0419, -0.2924,  1.2426,  1.2473, -0.0870,  1.6466]])), ('linear.bias', tensor([-0.6256]))])\n"
     ]
    }
   ],
   "source": [
    "MODEL_STATE_FILE_PATH=\"./Saved_models/state.pth\"\n",
    "\n",
    "# Save\n",
    "print(\"Saving ...\")\n",
    "torch.save(model.state_dict(),MODEL_STATE_FILE_PATH)\n",
    "print(\"Model Saved :D\")\n",
    "\n",
    "# Load\n",
    "print(\"Loading ...\")\n",
    "loaded_model_2=Model(n_input_features=n_input_features)\n",
    "loaded_model_2.load_state_dict(torch.load(MODEL_STATE_FILE_PATH))\n",
    "print(\"Model Loaded :D\")\n",
    "\n",
    "print(\"Old State:\",old_state)\n",
    "print(\"Saved Sate:\",model_trained_state)\n",
    "print(\"Loaded State:\",loaded_model_2.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples,n_features 10 6\n",
      "Training ......\n",
      "epoch: 10, loss = 18477.1152\n",
      "epoch: 20, loss = 18475.2148\n",
      "epoch: 30, loss = 18474.2852\n",
      "epoch: 40, loss = 18473.7305\n",
      "epoch: 50, loss = 18473.3633\n",
      "epoch: 60, loss = 18473.0996\n",
      "epoch: 70, loss = 18472.9023\n",
      "epoch: 80, loss = 18472.7461\n",
      "epoch: 90, loss = 18472.6230\n",
      "epoch: 100, loss = 18472.5195\n",
      "checkpoint {'epoch': 100, 'model_state': OrderedDict([('linear.weight', tensor([[ 2.2189, -0.6021,  2.3864,  1.9830,  0.3417,  3.0607]])), ('linear.bias', tensor([-1.4575]))]), 'optim_state': {'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}}\n",
      "Saving ...\n",
      "CheckPoint Saved :D\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 0)Prepare Data\n",
    "X_numpy,y_numpy=datasets.make_regression(n_samples=10,n_features=n_input_features,noise=20,random_state=1)\n",
    "\n",
    "x=torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0],1) # col vector\n",
    "\n",
    "n_samples,n_features=x.shape\n",
    "print(\"n_samples,n_features\",n_samples,n_features)\n",
    "\n",
    "\n",
    "print(\"Training ......\")\n",
    "learning_rate=0.01\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training \n",
    "num_epochs=100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass and loss\n",
    "    y_predicted=model(x)\n",
    "    loss=criterion(y_predicted,y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "checkpoint={\n",
    "    \"epoch\":100,\n",
    "    \"model_state\":model.state_dict(),\n",
    "    \"optim_state\":optimizer.state_dict()\n",
    "}\n",
    "print(\"checkpoint\",checkpoint)\n",
    "\n",
    "CHECKPOINT_PATH=\"./Saved_models/checkpoint.pth\"\n",
    "\n",
    "# Save\n",
    "print(\"Saving ...\")\n",
    "torch.save(checkpoint,CHECKPOINT_PATH)\n",
    "print(\"CheckPoint Saved :D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ...\n",
      "CheckPoint Loaded :D\n",
      "epoch: 10, loss = 18472.4336\n",
      "epoch: 20, loss = 18472.3633\n",
      "epoch: 30, loss = 18472.3008\n",
      "epoch: 40, loss = 18472.2461\n",
      "epoch: 50, loss = 18472.1992\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH=\"./Saved_models/checkpoint.pth\"\n",
    "\n",
    "# Load\n",
    "print(\"Loading ...\")\n",
    "loaded_checkpoint=torch.load(CHECKPOINT_PATH)\n",
    "print(\"CheckPoint Loaded :D\")\n",
    "\n",
    "\n",
    "epoch=loaded_checkpoint['epoch']\n",
    "\n",
    "model=Model(n_input_features=n_input_features)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0) \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "\n",
    "# Continue Training\n",
    "num_epochs=50\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass and loss\n",
    "    y_predicted=model(x)\n",
    "    loss=criterion(y_predicted,y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Parameters of Model\n",
      "OrderedDict([('linear.weight', tensor([[-0.2760,  0.0468, -0.2364, -0.3454, -0.1454, -0.1949]])), ('linear.bias', tensor([0.0373]))])\n",
      "n_samples,n_features 10 6\n",
      "Training ......\n",
      "epoch: 1, loss = 18625.9336\n",
      "old_state_model_gpu OrderedDict([('linear.weight', tensor([[-0.2760,  0.0468, -0.2364, -0.3454, -0.1454, -0.1949]])), ('linear.bias', tensor([0.0373]))])\n",
      "trained_model_gpu_state: OrderedDict([('linear.weight', tensor([[ 0.8653, -0.2219,  1.4056,  0.9043, -0.0538,  1.7966]])), ('linear.bias', tensor([-0.6246]))])\n",
      "\n",
      "\n",
      "Before Train Y tensor([0.1830], grad_fn=<SigmoidBackward0>)\n",
      "After Train Y: tensor([0.9855], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "n_input_features=6\n",
    "model_gpu=Model(n_input_features=n_input_features)\n",
    "\n",
    "print(\"Old Parameters of Model\")\n",
    "old_state_model_gpu = copy.deepcopy(model_gpu.state_dict())\n",
    "print(old_state_model_gpu)\n",
    "\n",
    "test_x=torch.tensor([0.0,0.0,3.0,0.0,5.0,0.5])\n",
    "model_gpu.eval()\n",
    "test_y=model_gpu(test_x)\n",
    "\n",
    "# 0)Prepare Data\n",
    "X_numpy,y_numpy=datasets.make_regression(n_samples=10,n_features=n_input_features,noise=20,random_state=1)\n",
    "\n",
    "x=torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y=y.view(y.shape[0],1) # col vector\n",
    "\n",
    "n_samples,n_features=x.shape\n",
    "print(\"n_samples,n_features\",n_samples,n_features)\n",
    "\n",
    "\n",
    "print(\"Training ......\")\n",
    "learning_rate=0.01\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model_gpu.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "# Training \n",
    "num_epochs=10\n",
    "model_gpu.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward Pass and loss\n",
    "    y_predicted=model_gpu(x)\n",
    "    loss=criterion(y_predicted,y)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch==0 or (epoch+1)%100==0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "trained_model_gpu_state=model_gpu.state_dict()\n",
    "print(\"old_state_model_gpu\",old_state_model_gpu)\n",
    "print(\"trained_model_gpu_state:\",trained_model_gpu_state)\n",
    "\n",
    "model_gpu.eval()\n",
    "print(\"\\n\\nBefore Train Y\",test_y)\n",
    "print('After Train Y:',model_gpu(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_model_gpu_state OrderedDict([('linear.weight', tensor([[ 0.8653, -0.2219,  1.4056,  0.9043, -0.0538,  1.7966]],\n",
      "       device='cuda:0')), ('linear.bias', tensor([-0.6246], device='cuda:0'))])\n",
      "Saved on GPU :D\n",
      "Loading CPU ....\n",
      "OrderedDict([('linear.weight', tensor([[ 0.8653, -0.2219,  1.4056,  0.9043, -0.0538,  1.7966]])), ('linear.bias', tensor([-0.6246]))])\n",
      "After Load(CPU) Y: tensor([0.9855], grad_fn=<SigmoidBackward0>)\n",
      "Loading GPU ....\n",
      "OrderedDict([('linear.weight', tensor([[ 0.8653, -0.2219,  1.4056,  0.9043, -0.0538,  1.7966]])), ('linear.bias', tensor([-0.6246]))])\n",
      "After Load(GPU) Y: tensor([0.9855], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "SAVE_FROM_GPU_PATH=\"./Saved_models/model_gpu.pth\"\n",
    "test_x=torch.tensor([0.0,0.0,3.0,0.0,5.0,0.5])\n",
    "\n",
    "\n",
    "# Save on GPU\n",
    "device=torch.device(\"cuda\")\n",
    "model_gpu.to(device)\n",
    "print(\"trained_model_gpu_state\",model_gpu.state_dict())\n",
    "torch.save(model_gpu.state_dict(),SAVE_FROM_GPU_PATH)\n",
    "print(\"Saved on GPU :D\")\n",
    "\n",
    "\n",
    "# Load on CPU\n",
    "print(\"Loading CPU ....\")\n",
    "device=torch.device('cpu')\n",
    "n_input_features=6\n",
    "model_gpu_loaded_cpu=Model(n_input_features=n_input_features)\n",
    "model_gpu_loaded_cpu.load_state_dict(torch.load(SAVE_FROM_GPU_PATH,map_location=device))\n",
    "print(model_gpu_loaded_cpu.state_dict())\n",
    "\n",
    "model_gpu_loaded_cpu.eval()\n",
    "print('After Load(CPU) Y:',model_gpu_loaded_cpu(test_x))\n",
    "\n",
    "# Load on GPU\n",
    "print(\"Loading GPU ....\")\n",
    "device=torch.device('cuda')\n",
    "n_input_features=6\n",
    "model_gpu_loaded_gpu=Model(n_input_features=n_input_features)\n",
    "model_gpu_loaded_gpu.load_state_dict(torch.load(SAVE_FROM_GPU_PATH))\n",
    "# model_gpu_loaded_gpu.load_state_dict(torch.load(SAVE_FROM_GPU_PATH,map_location=device)) # or just specify map_location which is the same :D\n",
    "print(model_gpu_loaded_gpu.state_dict())\n",
    "\n",
    "model_gpu_loaded_gpu.eval()\n",
    "print('After Load(GPU) Y:',model_gpu_loaded_gpu(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you don't specify the map_location parameter when using torch.load, PyTorch will attempt to load the model on the device where it was originally trained and saved. This can lead to issues if the model was trained and saved on a GPU, and you're trying to load it on a CPU or a different GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
